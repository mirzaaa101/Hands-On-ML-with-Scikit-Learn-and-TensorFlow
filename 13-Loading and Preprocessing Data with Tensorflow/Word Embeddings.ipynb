{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234cf4ee",
   "metadata": {},
   "source": [
    "### What Are Word Embeddings?\n",
    "\n",
    "A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
    "\n",
    "It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems.\n",
    "\n",
    "    \"One of the benefits of using dense and low-dimensional vectors is computational: the majority of neural network toolkits do not play well with very high-dimensional, sparse vectors. â€¦ The main benefit of the dense representations is generalization power: if we believe some features may provide similar clues, it is worthwhile to provide a representation that is able to capture these similarities.\"\n",
    "Ref:https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "\n",
    "One famous example to understand word embeddings is that if you give try: King-Man+Woman, computer will return a vector which is very close to the word 'Queen'. So we can say: King-Man+Woman=Queen. Similarly, Madrid-Spain+France=Paris.s\n",
    "\n",
    "However, word embeddings sometimes capture worst biases. For example, although they are correctly learn that Man is to King as Woman, they also seem to learn Doctor as Woman is to Nurse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e94361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ba7693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.02580125, -0.04535739],\n",
       "       [-0.04483942,  0.01235889],\n",
       "       [-0.02580125, -0.04535739]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=5, output_dim=2)\n",
    "embedding_layer(np.array([2,4,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d67d1dc",
   "metadata": {},
   "source": [
    "See, category 2 gets encoded twice as 2D vector, while category 4 gets encoded as [-0.04483942,  0.01235889]. Since the layer is not trained yet, these encoding are just random.\n",
    "\n",
    "For the categorical text attribute, you can simply chain a String Lookup layers and Embedding layer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8230928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 0.03129688, -0.04329207],\n",
       "       [ 0.02822297,  0.03865187],\n",
       "       [ 0.03129688, -0.04329207]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "ocean_prox = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(ocean_prox)\n",
    "\n",
    "lookup_and_embed = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=[], dtype=tf.string),\n",
    "    str_lookup_layer,\n",
    "    tf.keras.layers.Embedding(input_dim=str_lookup_layer.vocabulary_size(),output_dim=2)\n",
    "])\n",
    "\n",
    "lookup_and_embed(np.array([\"<1H OCEAN\", \"ISLAND\", \"<1H OCEAN\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff79051a",
   "metadata": {},
   "source": [
    "Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69cb7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 2s 3ms/step - loss: 0.1159 - val_loss: 0.0943\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0913 - val_loss: 0.0875\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0864 - val_loss: 0.0850\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0845 - val_loss: 0.0840\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0837 - val_loss: 0.0835\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train_num = np.random.rand(10_000, 8)\n",
    "X_train_cat = np.random.choice(ocean_prox, size=10_000)\n",
    "y_train = np.random.rand(10_000, 1)\n",
    "X_valid_num = np.random.rand(2_000, 8)\n",
    "X_valid_cat = np.random.choice(ocean_prox, size=2_000)\n",
    "y_valid = np.random.rand(2_000, 1)\n",
    "\n",
    "num_input = tf.keras.layers.Input(shape=[8], name=\"num\")\n",
    "cat_input = tf.keras.layers.Input(shape=[], dtype=tf.string, name=\"cat\")\n",
    "cat_embeddings = lookup_and_embed(cat_input) \n",
    "encoded_inputs = tf.keras.layers.concatenate([num_input, cat_embeddings])\n",
    "outputs = tf.keras.layers.Dense(1)(encoded_inputs)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[num_input, cat_input], outputs=[outputs])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "history = model.fit((X_train_num, X_train_cat), y_train, epochs=5, validation_data=((X_valid_num, X_valid_cat), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9fe37",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d2b57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n",
       "array([[2, 1, 0, 0],\n",
       "       [6, 2, 1, 2]], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\"to be\", \"!(to be)\", \"That's the question\", \"Be be, be.\"]\n",
    "\n",
    "text_vec_layer = tf.keras.layers.TextVectorization()\n",
    "text_vec_layer.adapt(train_data)\n",
    "\n",
    "text_vec_layer(['Be good!', 'Question: be or be?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b7adf",
   "metadata": {},
   "source": [
    "TextVectorization's adapt() method first converted the traning sentences to lowercase and removed punctuation, which is why \"Be\", \"be\" and \"be?\", all are encoded as \"be\"=2. Next, the sentences were split on whitespace, and the resulting words were sorted by the descending freequency, producing final vocabulary. When encoding sentences, unknown gets encoded as 1s. Lastly, since the first sentence is shorter than the second, it was padded with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83d5f63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(standardize=None, split=None, ragged=True)\n",
    "text_vec_layer.adapt(train_data)\n",
    "\n",
    "text_vec_layer(['Be good!', 'Question: be or be?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39ac4c",
   "metadata": {},
   "source": [
    "The TextVectorization class has so many option. For example, you can preserve the case and punctuation if you want by setting standardize=None, also you can prevent spliting by setting split=None. You can pass your own function to this argument. \n",
    "\n",
    "You can set output_sequence_length argument to ensure that all get cropped or padded to the desired length. Set ragged=True to get the ragged tensor instead of regular tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7907f1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "array([[0.96725637, 0.6931472 , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.96725637, 1.3862944 , 0.        , 0.        , 0.        ,\n",
       "        1.0986123 ]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(output_mode=\"tf_idf\")\n",
    "text_vec_layer.adapt(train_data)\n",
    "\n",
    "text_vec_layer(['Be good!', 'Question: be or be?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fdf48",
   "metadata": {},
   "source": [
    "Alternatively, you can set output_model argument to \"multi_hot\" or \"count\" to get the corresponding encodings. However, simply counting words is not ideal: words like \"to\" and \"the\" are so frequent that they are hardly matter at all. Whereas rarer words such as \"basketball\" are much informative. So, it is usally prefer to set to \"tf_idf\" instead of \"multi_hot\" or \"count\". \"tf_idf\" stands for term-frequency x inverse-document-frequnency (TF-IDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0158e",
   "metadata": {},
   "source": [
    "### Using Pretrained Language Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df8bd397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25,  0.28,  0.01,  0.1 ,  0.14,  0.16,  0.25,  0.02,  0.07,\n",
       "         0.13, -0.19,  0.06, -0.04, -0.07,  0.  , -0.08, -0.14, -0.16,\n",
       "         0.02, -0.24,  0.16, -0.16, -0.03,  0.03, -0.14,  0.03, -0.09,\n",
       "        -0.04, -0.14, -0.19,  0.07,  0.15,  0.18, -0.23, -0.07, -0.08,\n",
       "         0.01, -0.01,  0.09,  0.14, -0.03,  0.03,  0.08,  0.1 , -0.01,\n",
       "        -0.03, -0.07, -0.1 ,  0.05,  0.31],\n",
       "       [-0.2 ,  0.2 , -0.08,  0.02,  0.19,  0.05,  0.22, -0.09,  0.02,\n",
       "         0.19, -0.02, -0.14, -0.2 , -0.04,  0.01, -0.07, -0.22, -0.1 ,\n",
       "         0.16, -0.44,  0.31, -0.1 ,  0.23,  0.15, -0.05,  0.15, -0.13,\n",
       "        -0.04, -0.08, -0.16, -0.1 ,  0.13,  0.13, -0.18, -0.04,  0.03,\n",
       "        -0.1 , -0.07,  0.07,  0.03, -0.08,  0.02,  0.05,  0.07, -0.14,\n",
       "        -0.1 , -0.18, -0.13, -0.04,  0.15]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
    "sentence_embeddings.numpy().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e55e6",
   "metadata": {},
   "source": [
    "The hub.KerasLayer layer downloads the module from the given url. It takes strings as input and encodes each one of as a single vector (50 dimentions, in this case)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

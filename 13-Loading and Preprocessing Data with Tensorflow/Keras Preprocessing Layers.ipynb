{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc997670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26e089",
   "metadata": {},
   "source": [
    "### The Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed09f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 4.7883 - val_loss: 4.9976\n",
      "Epoch 2/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.6712 - val_loss: 1.9839\n",
      "Epoch 3/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9331 - val_loss: 1.1643\n"
     ]
    }
   ],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "model = tf.keras.models.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss = \"mse\",\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    ")\n",
    "\n",
    "norm_layer.adapt(X_train) # computes the mean and verience of each feature\n",
    "history = model.fit(X_train, y_train, epochs=3, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54760f57",
   "metadata": {},
   "source": [
    "Since we included the Normalization layer inside the model, we can now deploy this model for production. No need to worry about the Normalization again. But this method makes the model training slower, we can also do in an another way which will make the training faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a8b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(X_train)\n",
    "\n",
    "X_train_scaled = norm_layer(X_train)\n",
    "X_valid_scaled = norm_layer(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8153de6",
   "metadata": {},
   "source": [
    "We can now train our model on the scaled data, this time without thr Normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efd402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 4.9627 - val_loss: 7.4956\n",
      "Epoch 2/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3889 - val_loss: 2.1662\n",
      "Epoch 3/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8029 - val_loss: 1.5392\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=3, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33279528",
   "metadata": {},
   "source": [
    "This method should speed up the training a bit. But this time the model won't process Normalization it's inputs when we deploy the model. To fix this we need to create a new model that wraps both adapted Normalization and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd58052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n",
       "array([[0.8812941],\n",
       "       [1.4023672],\n",
       "       [2.0182023]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = tf.keras.Sequential([norm_layer,model])\n",
    "\n",
    "X_new = X_test[:3]\n",
    "y_pred = final_model(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087d6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Normalization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MyNormalization(tf.keras.layers.Layer):\n",
    "    def adapt(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0, keepdims=True)\n",
    "        self.std_ = np.std(X, axis=0, keepdims=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        return (inputs - self.mean_) / (self.std_ + eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f396e5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 4.2011 - val_loss: 6.5786\n",
      "Epoch 2/3\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.4751 - val_loss: 1.2005\n",
      "Epoch 3/3\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8409 - val_loss: 0.6763\n"
     ]
    }
   ],
   "source": [
    "my_norm = MyNormalization()\n",
    "my_norm.adapt(X_train)\n",
    "\n",
    "X_train_scaled = my_norm(X_train)\n",
    "X_valid_scaled = my_norm(X_valid)\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=3, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0cab0",
   "metadata": {},
   "source": [
    "### The Discretization Layer\n",
    "Map numerical feature to catagorical feature by mapping range values (bins). This is sometimes useful, especially when any feature has highly non-lenear relationship with the target. The following example convert the code into three catagories: less than 18, 18 to 50, 50 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a442a5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = tf.constant([[10.],[93.],[18.],[37.],[5.]])\n",
    "discretize_layer = tf.keras.layers.Discretization(bin_boundaries=[18., 50.])\n",
    "\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab68387",
   "metadata": {},
   "source": [
    "We could also set the number of bins, and then call the adapt() method to let it find the appropriate bin boundaries. For instance, we if set num_bins=3, then bin boundaries will be located at 33 and 66 parcentile (in this example, at the values 10 and 37)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c04e2e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0]], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretize_layer = tf.keras.layers.Discretization(num_bins=3)\n",
    "discretize_layer.adapt(age)\n",
    "\n",
    "age_categories = discretize_layer(age)\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc993b7",
   "metadata": {},
   "source": [
    "### The CategoryEncoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33317907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 3), dtype=float32, numpy=\n",
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_layer = tf.keras.layers.CategoryEncoding(num_tokens=3)\n",
    "onehot_layer(age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c6f71",
   "metadata": {},
   "source": [
    "To encode more than one categorical feature at a time which only make sense if they all use same categories, the CategoryEncoding class will perform multi-hot encoding by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eace9f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_age_categories = np.array([[1,0], [2,2], [2,0]])\n",
    "onehot_layer(two_age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352051de",
   "metadata": {},
   "source": [
    "### The StringLookup Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92167949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[3, 1, 1, 2]], dtype=int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = [\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]\n",
    "\n",
    "str_lookup_layer = tf.keras.layers.StringLookup()\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Auckland\", \"Paris\", \"Paris\", \"San Francisco\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95048628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[1, 3, 3, 0]], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer([[\"Paris\", \"Auckland\", \"Auckland\", \"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4116981",
   "metadata": {},
   "source": [
    "The known categoeies are start mapping from 1 (most frequent category to least category). Unknown categoeies are mapped as 0, in this case 'Montreal' is mapped as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "336f2d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 367 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000002CEF0254E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Auckland\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3522805",
   "metadata": {},
   "source": [
    "If the training set is very large, it may be convenient to adapt the layer to just a random subset of the training set. In that case, the layers adapt() method may miss some of rarer categories. By default, it will map them all to 0 which will make them undistingguishable. To reduce this risk, we can set num_oov_indices to an integer greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e8dadf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 368 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000002CEF1C01550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int64, numpy=\n",
       "array([[5],\n",
       "       [7],\n",
       "       [4],\n",
       "       [3],\n",
       "       [4]], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lookup_layer = tf.keras.layers.StringLookup(num_oov_indices=5)\n",
    "str_lookup_layer.adapt(cities)\n",
    "str_lookup_layer([[\"Paris\"], [\"Auckland\"], [\"Foo\"], [\"Bar\"], [\"Baz\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1ad23",
   "metadata": {},
   "source": [
    "Since there are five OOV buckets, the first known category's (Paris) id is now 5. But \"Foo\", \"Baz\" and \"Bar\" are unknown, so thay get mapped to one of the OOV buckets. \"Bar\" gets mapped to id 3, but unfortunately \"Foo\", \"Baz\" gets mapped in the same bucket (4). This is called a hashing collision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb483a3",
   "metadata": {},
   "source": [
    "### The Hashing Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21b4bc00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       "array([[0],\n",
       "       [1],\n",
       "       [9],\n",
       "       [1]], dtype=int64)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing_layer = tf.keras.layers.Hashing(num_bins=10)\n",
    "hashing_layer([[\"Paris\"], [\"Tokyo\"], [\"Auckland\"], [\"Montreal\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc5b72",
   "metadata": {},
   "source": [
    "We again got hashing collision, \"Tokyo\" and \"Montreal\" get mapped in the same bucket. So, its better to get stick to the StringLookup layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

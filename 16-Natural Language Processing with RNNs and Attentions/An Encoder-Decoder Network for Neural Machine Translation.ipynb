{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aca249fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a1e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVe.\n",
      "Go.\tVete.\n",
      "Go.\tVaya.\n",
      "Go.\tVáyase.\n",
      "Hi.\tHola.\n",
      "Run!\t¡Corre!\n",
      "Run.\tCorred.\n",
      "Who?\t¿Quién?\n",
      "Fire!\t¡Fuego!\n",
      "Fire!\t¡Incendio!\n",
      "Fire!\t¡Disparad!\n",
      "Help!\t¡Ayuda!\n",
      "Help!\t¡Socorro! ¡Auxilio!\n",
      "Help!\t¡Auxilio!\n",
      "Jump!\t¡Salta!\n",
      "Jump.\tSalte.\n",
      "Stop!\t¡Parad!\n",
      "Stop!\t¡Para!\n",
      "Stop!\t¡Pare!\n",
      "Wait!\t¡Espera!\n",
      "Wait.\tEsperen.\n",
      "Go on.\tContinúa.\n",
      "Go on.\tContinúe.\n",
      "Hello!\tHola.\n",
      "I ran.\tCorrí.\n",
      "I ran.\tCorría.\n",
      "I try.\tLo intento.\n",
      "I won!\t¡He ganado!\n",
      "Oh no!\t¡Oh, no!\n",
      "Relax.\tTomátelo con soda.\n",
      "Smile.\tSonríe.\n",
      "Attack!\t¡Al ataque!\n",
      "Attack!\t¡Atacad!\n",
      "Ge\n"
     ]
    }
   ],
   "source": [
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets/\", extract=True)\n",
    "\n",
    "extracted_path = Path(path).with_name(\"spa-eng\") / \"spa.txt\"\n",
    "text = extracted_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "852857d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.seed(42) \n",
    "np.random.shuffle(pairs)\n",
    "\n",
    "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e4c1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How boring! => Qué aburrimiento!\n",
      "I love sports. => Adoro el deporte.\n",
      "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef3fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size, output_sequence_length=max_length)\n",
    "\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff828c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'i', 'to', 'you', 'tom', 'a', 'is', 'he']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69a52522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'startofseq', 'endofseq', 'de', 'que', 'a', 'no', 'tom', 'la']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_es.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef0c6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "\n",
    "y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51f42c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc51c5b",
   "metadata": {},
   "source": [
    "#### Building Model Using Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "effa3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bee91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da162c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "446e855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "y_proba = output_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef55a9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3125/3125 [==============================] - 3327s 1s/step - loss: 2.9674 - accuracy: 0.4189 - val_loss: 3.0684 - val_accuracy: 0.4193\n",
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 3065s 981ms/step - loss: 1.8857 - accuracy: 0.5677 - val_loss: 2.5862 - val_accuracy: 0.4599\n",
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 3068s 982ms/step - loss: 1.4596 - accuracy: 0.6444 - val_loss: 2.4194 - val_accuracy: 0.4765\n",
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 3067s 981ms/step - loss: 1.2184 - accuracy: 0.6911 - val_loss: 2.3970 - val_accuracy: 0.4853\n",
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 3078s 985ms/step - loss: 1.0548 - accuracy: 0.7247 - val_loss: 2.4538 - val_accuracy: 0.4805\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit((X_train, X_train_dec), y_train, epochs=5, validation_data=((X_valid, X_valid_dec), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938ad68",
   "metadata": {},
   "source": [
    "#### Translating English to Spanish Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86e800a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_inx in range(max_length):\n",
    "        X = np.array([sentence_en])\n",
    "        X_dec = np.array([\"startofseq\" + translation])\n",
    "        \n",
    "        y_proba = model.predict((X, X_dec), verbose=0)[0, word_inx]\n",
    "        \n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        \n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "        \n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9ed7c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' te amo'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6d6b05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' me encanta el fútbol'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I love soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02686d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' voy a ir al colegio ahora mismo'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I am going to school right now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0185257",
   "metadata": {},
   "source": [
    "#### Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24db56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_state=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cad68b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1),  # short-term (0 & 2)\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)]  # long-term (1 & 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e14ab84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3125/3125 [==============================] - 2378s 756ms/step - loss: 2.6217 - accuracy: 0.4659 - val_loss: 2.8547 - val_accuracy: 0.4315\n",
      "Epoch 2/5\n",
      "3125/3125 [==============================] - 2367s 758ms/step - loss: 1.6265 - accuracy: 0.6139 - val_loss: 2.5133 - val_accuracy: 0.4666\n",
      "Epoch 3/5\n",
      "3125/3125 [==============================] - 2369s 758ms/step - loss: 1.2939 - accuracy: 0.6767 - val_loss: 2.3738 - val_accuracy: 0.4832\n",
      "Epoch 4/5\n",
      "3125/3125 [==============================] - 2371s 759ms/step - loss: 1.1069 - accuracy: 0.7145 - val_loss: 2.4069 - val_accuracy: 0.4800\n",
      "Epoch 5/5\n",
      "3125/3125 [==============================] - 2364s 756ms/step - loss: 0.9702 - accuracy: 0.7427 - val_loss: 2.3451 - val_accuracy: 0.4887\n"
     ]
    }
   ],
   "source": [
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "y_proba = output_layer(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=[y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit((X_train, X_train_dec), y_train, epochs=5, validation_data=((X_valid, X_valid_dec), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af330c6e",
   "metadata": {},
   "source": [
    "#### Beam Search: A very basic implementation of beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b460e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(sentence_en, beam_width, verbose=False):\n",
    "    X = np.array([sentence_en])  # encoder input\n",
    "    X_dec = np.array([\"startofseq\"])  # decoder input\n",
    "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
    "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
    "    top_translations = [  # list of best (log_proba, translation)\n",
    "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
    "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
    "    ]\n",
    "    \n",
    "    # displays the top first words in verbose mode\n",
    "    if verbose:\n",
    "        print(\"Top first words:\", top_translations)\n",
    "\n",
    "    for idx in range(1, max_length):\n",
    "        candidates = []\n",
    "        for log_proba, translation in top_translations:\n",
    "            if translation.endswith(\"endofseq\"):\n",
    "                candidates.append((log_proba, translation))\n",
    "                continue  # translation is finished, so don't try to extend it\n",
    "            X = np.array([sentence_en])  # encoder input\n",
    "            X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
    "            for word_id, word_proba in enumerate(y_proba):\n",
    "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
    "                candidates.append((log_proba + np.log(word_proba),\n",
    "                                   f\"{translation} {word}\"))\n",
    "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
    "\n",
    "        # displays the top translation so far in verbose mode\n",
    "        if verbose:\n",
    "            print(\"Top translations so far:\", top_translations)\n",
    "\n",
    "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
    "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0f3b672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' amo a los perros y gatos'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_en = \"I love cats and dogs\"\n",
    "translate(sentence_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29dd88af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Top first words: [(-0.6904845, 'amo'), (-1.4752278, 'me'), (-2.326438, 'conozco')]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top translations so far: [(-0.91566813, 'amo a'), (-1.8409061, 'me [UNK]'), (-2.6258357, 'conozco a')]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top translations so far: [(-1.0702174, 'amo a los'), (-2.1794553, 'me [UNK] los'), (-2.8969083, 'conozco a los')]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top translations so far: [(-1.7575715, 'amo a los perros'), (-1.8192847, 'amo a los gatos'), (-2.6674814, 'me [UNK] los gatos')]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Top translations so far: [(-1.9208897, 'amo a los gatos y'), (-2.0812309, 'amo a los perros y'), (-2.7925155, 'me [UNK] los gatos y')]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top translations so far: [(-2.353173, 'amo a los perros y gatos'), (-2.3917155, 'amo a los gatos y perros'), (-2.8942475, 'me [UNK] los gatos y perros')]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Top translations so far: [(-2.3587132, 'amo a los perros y gatos endofseq'), (-2.39914, 'amo a los gatos y perros endofseq'), (-2.9046323, 'me [UNK] los gatos y perros endofseq')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'amo a los perros y gatos'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(sentence_en, beam_width=3, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
